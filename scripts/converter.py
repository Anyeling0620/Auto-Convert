import json
import os
import uuid
import hashlib
import time
import requests
import random
import re
from docx import Document
from zhipuai import ZhipuAI
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ================= ğŸ›¡ï¸ å…¨å±€é…ç½®åŒºåŸŸ =================
INPUT_DIR = "input"
OUTPUT_DIR = "output"
# æ–‡ä»¶åå°†è‡ªåŠ¨ç”Ÿæˆ output{N}.json

ZHIPU_API_KEY = os.getenv("ZHIPU_API_KEY")
PUSHPLUS_TOKEN = os.getenv("PUSHPLUS_TOKEN")

AI_MODEL_NAME = "glm-4-flash"
MAX_WORKERS = 16  # é«˜å¹¶å‘
CHUNK_SIZE = 2000  # é€‚ä¸­åˆ‡ç‰‡
OVERLAP = 200  # å¿…è¦çš„é‡å é˜²æ­¢åˆ‡æ–­é¢˜ç›®
MAX_RETRIES = 5  # é¥±å’Œå¼é‡è¯•
API_TIMEOUT = 60  # å•æ¬¡è¯·æ±‚è¶…æ—¶æ§åˆ¶
# =================================================

if not ZHIPU_API_KEY:
    print("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ° ZHIPU_API_KEY")
    exit(1)

client = ZhipuAI(api_key=ZHIPU_API_KEY)

# æ‰©å±•ç™½åå•ï¼šåŒ…å«åŒ»å­¦ã€ç†å·¥ã€æ–‡å²
STANDARD_CATEGORIES = {
    "A1å‹é¢˜", "A2å‹é¢˜", "B1å‹é¢˜", "Xå‹é¢˜", "é…ä¼é¢˜", "ç—…ä¾‹åˆ†æé¢˜",
    "å•é€‰é¢˜", "å¤šé€‰é¢˜", "åˆ¤æ–­é¢˜", "å¡«ç©ºé¢˜",
    "åè¯è§£é‡Šé¢˜", "ç®€ç­”é¢˜", "è®ºè¿°é¢˜",
    "è®¡ç®—é¢˜", "è¯æ˜é¢˜", "ç¼–ç¨‹é¢˜", "åº”ç”¨é¢˜", "ç»¼åˆé¢˜"
}


def get_next_output_filename():
    """ğŸ›¡ï¸ è‡ªåŠ¨è·å–ä¸‹ä¸€ä¸ªæ–‡ä»¶åï¼Œé˜²æ­¢è¦†ç›–"""
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    existing_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith("output") and f.endswith(".json")]
    max_index = 0
    for f in existing_files:
        match = re.search(r'output(\d+)\.json', f)
        if match:
            idx = int(match.group(1))
            if idx > max_index:
                max_index = idx
    return os.path.join(OUTPUT_DIR, f"output{max_index + 1}.json")


def send_notification(title, content):
    if not PUSHPLUS_TOKEN: return
    try:
        requests.post("http://www.pushplus.plus/send", json={
            "token": PUSHPLUS_TOKEN, "title": title, "content": content, "template": "html"
        }, timeout=5)
    except:
        pass


def read_docx(file_path):
    """ğŸ›¡ï¸ é²æ£’çš„æ–‡ä»¶è¯»å–"""
    if not os.path.exists(file_path): return ""
    try:
        doc = Document(file_path)
        # è¿‡æ»¤ç©ºè¡Œï¼Œå‡å°‘ Token æ¶ˆè€—
        return "\n".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return ""


def get_chunks(text, chunk_size, overlap):
    chunks = []
    start = 0
    total_len = len(text)
    while start < total_len:
        end = min(start + chunk_size, total_len)
        chunks.append(text[start:end])
        if end == total_len: break
        start = end - overlap
    return chunks


def normalize_category(raw_cat):
    """ğŸ›¡ï¸ å¼ºåŠ›å½’ä¸€åŒ–ï¼šä¸ç®¡ AI è¾“å‡ºä»€ä¹ˆï¼Œå¼ºè¡Œæ˜ å°„åˆ°æ ‡å‡†åº“"""
    if not raw_cat: return "ç»¼åˆé¢˜"
    cat = raw_cat.strip()

    # 1. ä¼˜å…ˆåŒ»å­¦æœ¯è¯­
    if "A1" in cat: return "A1å‹é¢˜"
    if "A2" in cat: return "A2å‹é¢˜"
    if "B1" in cat or "é…ä¼" in cat: return "B1å‹é¢˜"
    if "Xå‹" in cat: return "Xå‹é¢˜"
    if "ç—…ä¾‹" in cat or "ç—…æ¡ˆ" in cat: return "ç—…ä¾‹åˆ†æé¢˜"

    # 2. é€šç”¨æ˜ å°„
    if "å¤šé€‰" in cat or "ä¸å®šé¡¹" in cat: return "å¤šé€‰é¢˜"
    if "å•é€‰" in cat: return "å•é€‰é¢˜"
    if "åˆ¤æ–­" in cat or "æ˜¯é" in cat: return "åˆ¤æ–­é¢˜"
    if "å¡«ç©º" in cat: return "å¡«ç©ºé¢˜"
    if "åè¯" in cat: return "åè¯è§£é‡Šé¢˜"
    if "ç®€ç­”" in cat or "é—®ç­”" in cat: return "ç®€ç­”é¢˜"
    if "è®ºè¿°" in cat: return "è®ºè¿°é¢˜"

    # 3. ç†å·¥ç‰¹è‰²
    if "è®¡ç®—" in cat: return "è®¡ç®—é¢˜"
    if "è¯æ˜" in cat: return "è¯æ˜é¢˜"
    if "ç¼–ç¨‹" in cat or "ä»£ç " in cat: return "ç¼–ç¨‹é¢˜"
    if "åº”ç”¨" in cat or "è®¾è®¡" in cat: return "åº”ç”¨é¢˜"

    if cat in STANDARD_CATEGORIES: return cat
    if not cat.endswith("é¢˜"): return cat + "é¢˜"
    return cat


def repair_json(json_str):
    """ğŸ›¡ï¸ JSON å¼ºåŠ›ä¿®å¤æ‰‹æœ¯"""
    json_str = json_str.strip()

    # 1. å»é™¤ Markdown ä»£ç å—
    if "```json" in json_str:
        json_str = json_str.split("```json")[1].split("```")[0]
    elif "```" in json_str:
        json_str = json_str.split("```")[1].split("```")[0]

    json_str = json_str.strip()

    # 2. å°è¯•ä¿®å¤æˆªæ–­çš„æ•°ç»„
    # å¦‚æœä¸æ˜¯ä»¥ ] ç»“å°¾ï¼Œå°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ª } å¹¶è¡¥ä¸Š ]
    if not json_str.endswith("]"):
        last_brace = json_str.rfind("}")
        if last_brace != -1:
            json_str = json_str[:last_brace + 1] + "]"
        else:
            # æç«¯æƒ…å†µï¼šè¿ä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡éƒ½æ²¡æœ‰ï¼Œè¿”å›ç©ºæ•°ç»„
            return "[]"

    return json_str


def extract_global_answers(full_text):
    print("   ğŸ” [Step 1] æ‰«ææ–‡æ¡£å‚è€ƒç­”æ¡ˆ...")
    # æˆªå–å…¨æ–‡æ‰«æ (Flashæ”¯æŒ128k contextï¼Œç›´æ¥ä¸Š)
    safe_text = full_text[:100000]
    prompt = """
    ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æå¸ˆã€‚è¯·æ‰«ææœ¬æ–‡æ¡£ï¼Œæå–æ‰€æœ‰â€œå‚è€ƒç­”æ¡ˆâ€éƒ¨åˆ†ã€‚
    ã€è¦æ±‚ã€‘
    1. å¿½ç•¥é¢˜ç›®å†…å®¹ï¼Œ**åªæå–ç­”æ¡ˆ**ã€‚
    2. è¾“å‡ºæ ¼å¼ä¸ºçº¯æ–‡æœ¬åˆ—è¡¨ï¼ˆå¦‚ï¼š1.A 2.B 3.C ...ï¼‰ã€‚
    3. å¦‚æœæ‰¾ä¸åˆ°é›†ä¸­ç­”æ¡ˆï¼Œè¿”å›â€œæ— â€ã€‚
    """
    try:
        response = client.chat.completions.create(
            model=AI_MODEL_NAME,
            messages=[{"role": "user", "content": prompt + "\n\n" + safe_text}],
            temperature=0.1,
            timeout=120
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"   âš ï¸ ç­”æ¡ˆæ‰«æå¤±è´¥: {e}")
        return ""


def process_single_chunk(args):
    chunk, index, total, answer_key = args

    # =================================================================
    # âš¡ ä¸¥è°¨çº§ Prompt (ä¸­æ–‡ç‰ˆ) - ä¸“æ²» AI å¹»è§‰å’Œæ ¼å¼é”™è¯¯
    # =================================================================
    prompt = f"""
    [ç³»ç»Ÿè§’è‰²]
    ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼éµå¾ªæŒ‡ä»¤çš„â€œé€šç”¨è¯•é¢˜æ•°æ®æ¸…æ´—å¼•æ“â€ã€‚ä½ **ä¸æ˜¯**èŠå¤©æœºå™¨äººã€‚
    ä½ çš„ä»»åŠ¡æ˜¯å°†éç»“æ„åŒ–æ–‡æœ¬è½¬æ¢ä¸ºç¬¦åˆä»¥ä¸‹ Schema çš„ JSON æ•°ç»„ã€‚

    [è¾“å…¥ä¸Šä¸‹æ–‡ï¼šå‚è€ƒç­”æ¡ˆåº“]
    (å½“é¢˜ç›®ä¸­æ²¡æœ‰è‡ªå¸¦ç­”æ¡ˆæ—¶ï¼Œè¯·æŸ¥è¯¢æ­¤åº“)
    -----------------------------------
    {answer_key[:5000]}
    -----------------------------------

    [æ ¸å¿ƒå¤„ç†å®ˆåˆ™]
    1. **è¾¹ç•Œä¸¢å¼ƒåŸåˆ™**ï¼šè¾“å…¥æ–‡æœ¬æ˜¯ä¸€ä¸ªåˆ‡ç‰‡ã€‚å¦‚æœåˆ‡ç‰‡å¼€å¤´çš„ç¬¬ä¸€å¥è¯æ˜¯ä¸å®Œæ•´çš„ï¼ˆä¾‹å¦‚åªæœ‰é€‰é¡¹æ²¡æœ‰é¢˜å¹²ï¼‰ï¼Œæˆ–è€…åˆ‡ç‰‡æœ«å°¾æœ€åä¸€å¥è¯ä¸å®Œæ•´ï¼Œ**å¿…é¡»ç›´æ¥ä¸¢å¼ƒ**ã€‚ä¸¥ç¦è„‘è¡¥æ®‹ç¼ºå†…å®¹ã€‚
    2. **ç­”æ¡ˆåŒ¹é…ä¼˜å…ˆçº§**ï¼š
       - **ä¼˜å…ˆçº§ 1**ï¼šé¢˜ç›®æ–‡æœ¬ä¸­è‡ªå¸¦çš„ç­”æ¡ˆï¼ˆä¾‹å¦‚æ‹¬å·å†…ã€é¢˜å¹²æœ«å°¾ã€é€‰é¡¹ä¸‹æ–¹çš„â€œã€ç­”æ¡ˆã€‘â€ï¼‰ã€‚
       - **ä¼˜å…ˆçº§ 2**ï¼šæ ¹æ®ã€é¢˜å·ã€‘å»ä¸Šæ–¹çš„ [å‚è€ƒç­”æ¡ˆåº“] ä¸­æŸ¥æ‰¾ã€‚
       - **ä¼˜å…ˆçº§ 3**ï¼šå¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œ`answer` å­—æ®µç•™ç©ºå­—ç¬¦ä¸² ""ã€‚**ä¸¥ç¦éšæœºç”Ÿæˆç­”æ¡ˆã€‚**
    3. **å†…å®¹æ¸…æ´—**ï¼š
       - ç§»é™¤ `content` å­—æ®µå¼€å¤´çš„é¢˜å·ï¼ˆå¦‚ "1. "ï¼‰ã€‚
       - ç§»é™¤ `options` ä¸­ `text` å­—æ®µå¼€å¤´çš„æ ‡ç­¾ï¼ˆå¦‚ "A. "ï¼‰ï¼Œæ ‡ç­¾æ”¾å…¥ `label`ã€‚

    [å­¦ç§‘é¢˜å‹æ˜ å°„è¡¨ (Category Inference)]
    è¯·æ ¹æ®é¢˜ç›®å†…å®¹ç‰¹å¾ï¼Œä»ä¸‹è¡¨ä¸­é€‰æ‹©æœ€å‡†ç¡®çš„åˆ†ç±»å¡«å…¥ `category`ï¼š
    - **åŒ»å­¦ç±»**ï¼š
      - 5ä¸ªé€‰é¡¹(A-E)å•é€‰ -> "A1å‹é¢˜" æˆ– "A2å‹é¢˜"
      - é…ä¼é¢˜/å…±ç”¨é¢˜å¹² -> "B1å‹é¢˜"
      - å¤šé€‰é¢˜ -> "Xå‹é¢˜"
      - ç—…ä¾‹æè¿° -> "ç—…ä¾‹åˆ†æé¢˜"
    - **ç†å·¥/è®¡ç®—æœºç±»**ï¼š
      - ä»£ç å¡«ç©º/ç®—æ³•è®¾è®¡ -> "ç¼–ç¨‹é¢˜"
      - æ•°å€¼è®¡ç®—/å…¬å¼æ¨å¯¼ -> "è®¡ç®—é¢˜"
      - è¯æ˜/æ¨å¯¼ -> "è¯æ˜é¢˜"
    - **é€šç”¨ç±»**ï¼š
      - 4ä¸ªé€‰é¡¹å•é€‰ -> "å•é€‰é¢˜"
      - å¤šä¸ªæ­£ç¡®ç­”æ¡ˆ -> "å¤šé€‰é¢˜"
      - åˆ¤æ–­æ­£è¯¯(å¯¹/é”™) -> "åˆ¤æ–­é¢˜"
      - ä¸‹åˆ’çº¿å¡«ç©º -> "å¡«ç©ºé¢˜"
      - æ— é€‰é¡¹é—®ç­” -> "ç®€ç­”é¢˜"

    [JSON è¾“å‡ºç»“æ„ (Strict Schema)]
    å¿…é¡»è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œä¸è¦åŒ…å« ```json æ ‡è®°ã€‚
    [
      {{
        "category": "String (è§æ˜ å°„è¡¨)",
        "type": "Enum (SINGLE_CHOICE / MULTI_CHOICE / TRUE_FALSE / FILL_BLANK / ESSAY)",
        "content": "String (æ¸…æ´—åçš„é¢˜å¹²)",
        "options": [
           {{"label": "A", "text": "..."}},
           {{"label": "B", "text": "..."}}
        ],
        "answer": "String (ä¾‹å¦‚ 'A', 'ABC', 'True', 'ä»£ç ...')",
        "analysis": ""
      }}
    ]

    [å¾…å¤„ç†æ–‡æœ¬]
    {chunk}
    """
    # =================================================================

    for attempt in range(MAX_RETRIES):
        try:
            # åŠ¨æ€æ¸©åº¦ï¼šé‡è¯•æ¬¡æ•°è¶Šå¤šï¼Œæ¸©åº¦ç•¥å¾®å‡é«˜é˜²æ­»å¾ªç¯
            temp = 0.0 if attempt < 2 else 0.1

            response = client.chat.completions.create(
                model=AI_MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=temp,
                top_p=0.7,
                max_tokens=4000,
                timeout=API_TIMEOUT
            )
            content = response.choices[0].message.content

            # ğŸ›¡ï¸ æ·±åº¦æ¸…æ´—ä¸ä¿®å¤
            content = repair_json(content)

            try:
                res = json.loads(content)
                if isinstance(res, list): return res
                if isinstance(res, dict): return [res]
                # å¦‚æœè§£æå‡ºæ¥æ˜¯ç©ºæˆ–è€…å…¶ä»–ç±»å‹ï¼Œè§†ä¸ºå¤±è´¥
                return []
            except json.JSONDecodeError:
                if attempt == MAX_RETRIES - 1:
                    print(f"      âŒ Chunk {index + 1} JSON è§£æå½»åº•å¤±è´¥ã€‚")
                continue

        except Exception as e:
            # æŒ‡æ•°é€€é¿ï¼š1s, 2s, 4s...
            wait_time = (2 ** attempt) + random.uniform(0, 1)
            time.sleep(wait_time)

    return []


def main():
    start_time = time.time()

    if not os.path.exists(INPUT_DIR): os.makedirs(INPUT_DIR)
    docx_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(".docx")]

    if not docx_files:
        print("âŒ input ç›®å½•ä¸ºç©ºã€‚")
        return

    # 1. ç¡®å®šè¾“å‡ºæ–‡ä»¶å
    target_output_file = get_next_output_filename()
    print(f"ğŸš€ ä»»åŠ¡å¯åŠ¨ | å°†ç”Ÿæˆæ–‡ä»¶: {target_output_file} | çº¿ç¨‹æ•°: {MAX_WORKERS}")

    all_questions = []

    for filename in docx_files:
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        raw_text = read_docx(os.path.join(INPUT_DIR, filename))
        if not raw_text: continue

        global_answers = extract_global_answers(raw_text)
        chunks = get_chunks(raw_text, CHUNK_SIZE, OVERLAP)

        tasks_args = [(chunk, i, len(chunks), global_answers) for i, chunk in enumerate(chunks)]
        chunk_added = 0

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # ä½¿ç”¨ tqdm åŒ…è£… executor.map å®ç°è¿›åº¦æ¡
            results = list(tqdm(executor.map(process_single_chunk, tasks_args), total=len(chunks), unit="åˆ‡ç‰‡"))

            for items in results:
                if items:
                    for item in items:
                        # è¡¥å…¨å…ƒæ•°æ®
                        item['id'] = str(uuid.uuid4())
                        item['number'] = len(all_questions) + 1
                        item['chapter'] = filename.replace(".docx", "")
                        item['category'] = normalize_category(item.get('category', 'ç»¼åˆé¢˜'))
                        if 'analysis' not in item: item['analysis'] = ""

                        all_questions.append(item)
                        chunk_added += 1

        print(f"   âœ… æœ¬æ–‡ä»¶æå–: {chunk_added} é“")

    # æ„å»ºæœ€ç»ˆ JSON
    final_json = {
        "version": "Universal-V2.0",
        "source": "GLM-4-Flash-Auto",
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "total_count": len(all_questions),
        "data": all_questions
    }

    with open(target_output_file, 'w', encoding='utf-8') as f:
        json.dump(final_json, f, ensure_ascii=False, indent=2)

    duration = time.time() - start_time
    msg = f"ç”Ÿæˆå®Œæˆï¼\nè€—æ—¶: {duration:.1f}s\næ–‡ä»¶: {target_output_file}\né¢˜æ•°: {len(all_questions)}"
    print(f"\nâœ¨ {msg}")

    # å°†æ–‡ä»¶åå†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œä¼ é€’ç»™ Validator
    with open("last_generated_file.txt", "w") as f:
        f.write(target_output_file)


if __name__ == "__main__":
    main()