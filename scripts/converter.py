import json
import os
import uuid
import hashlib
import time
import requests
import random
import re
from docx import Document
from zhipuai import ZhipuAI
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm  # è¿›åº¦æ¡åº“

# ================= é…ç½®åŒºåŸŸ =================
INPUT_DIR = "input"
OUTPUT_DIR = "output"
OUTPUT_FILE = "questions_full.json"

ZHIPU_API_KEY = os.getenv("ZHIPU_API_KEY")
PUSHPLUS_TOKEN = os.getenv("PUSHPLUS_TOKEN")

AI_MODEL_NAME = "glm-4-flash"

# ã€æé€Ÿæ¨¡å¼é…ç½®ã€‘
MAX_WORKERS = 16       # å¹¶å‘æ•°æ‹‰åˆ° 16 (Flashæ¨¡å‹QPSå¾ˆé«˜ï¼Œå®Œå…¨æ’‘å¾—ä½)
CHUNK_SIZE = 1500      # ä¿æŒ 1500 ä»¥é˜²æ­¢æˆªæ–­
OVERLAP = 200          # é‡å åŒºåŸŸ
MAX_RETRIES = 5        # å¤±è´¥é‡è¯•æ¬¡æ•°å¢åŠ åˆ° 5 æ¬¡

# ===========================================

if not ZHIPU_API_KEY:
    print("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ° ZHIPU_API_KEY")
    exit(1)

client = ZhipuAI(api_key=ZHIPU_API_KEY)

STANDARD_CATEGORIES = {
    "å•é€‰é¢˜", "å¤šé€‰é¢˜", "åˆ¤æ–­é¢˜", "å¡«ç©ºé¢˜", 
    "åè¯è§£é‡Šé¢˜", "ç®€ç­”é¢˜", "è®ºè¿°é¢˜", 
    "è®¡ç®—é¢˜", "è¯æ˜é¢˜", "åº”ç”¨é¢˜", "ç¼–ç¨‹é¢˜",
    "é…ä¼é¢˜", "æ¡ˆä¾‹åˆ†æé¢˜", "ç»¼åˆé¢˜"
}

def send_notification(title, content):
    if not PUSHPLUS_TOKEN: return
    try:
        requests.post("http://www.pushplus.plus/send", json={
            "token": PUSHPLUS_TOKEN, "title": title, "content": content, "template": "html"
        }, timeout=5)
    except: pass

def read_docx(file_path):
    if not os.path.exists(file_path): return ""
    try:
        doc = Document(file_path)
        return "\n".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])
    except: return ""

def get_chunks(text, chunk_size, overlap):
    chunks = []
    start = 0
    total_len = len(text)
    while start < total_len:
        end = min(start + chunk_size, total_len)
        chunks.append(text[start:end])
        if end == total_len: break
        start = end - overlap
    return chunks

def generate_fingerprint(q_obj):
    raw = q_obj.get("content", "") + str(q_obj.get("options", ""))
    return hashlib.md5(raw.encode('utf-8')).hexdigest()

def normalize_category(raw_cat):
    if not raw_cat: return "ç»¼åˆé¢˜"
    cat = raw_cat.strip()
    
    if "å¤šé€‰" in cat or "ä¸å®šé¡¹" in cat: return "å¤šé€‰é¢˜"
    if "å•é€‰" in cat or "A1" in cat or "A2" in cat: return "å•é€‰é¢˜"
    if "åˆ¤æ–­" in cat or "æ˜¯é" in cat: return "åˆ¤æ–­é¢˜"
    if "å¡«ç©º" in cat: return "å¡«ç©ºé¢˜"
    if "é…ä¼" in cat or "è¿çº¿" in cat or "B1" in cat: return "é…ä¼é¢˜"
    if "åè¯" in cat: return "åè¯è§£é‡Šé¢˜"
    if "è®ºè¿°" in cat: return "è®ºè¿°é¢˜"
    if "ç®€ç­”" in cat or "é—®ç­”" in cat: return "ç®€ç­”é¢˜"
    if "è®¡ç®—" in cat: return "è®¡ç®—é¢˜"
    if "ç¼–ç¨‹" in cat or "ä»£ç " in cat: return "ç¼–ç¨‹é¢˜"
    if "åº”ç”¨" in cat: return "åº”ç”¨é¢˜"
    if "è¯æ˜" in cat: return "è¯æ˜é¢˜"
    if "æ¡ˆä¾‹" in cat or "ç—…ä¾‹" in cat: return "æ¡ˆä¾‹åˆ†æé¢˜"
    
    if cat in STANDARD_CATEGORIES: return cat
    if not cat.endswith("é¢˜"): return cat + "é¢˜"
    return cat

def repair_json(json_str):
    """JSON ä¿®å¤æ‰‹æœ¯"""
    json_str = json_str.strip()
    if not json_str.endswith("]"):
        last_brace = json_str.rfind("}")
        if last_brace != -1:
            json_str = json_str[:last_brace+1] + "]"
    return json_str

def extract_global_answers(full_text):
    print("   ğŸ” [Step 1] æ‰«æå…¨å±€å‚è€ƒç­”æ¡ˆ...")
    prompt = "è¯·æå–æ–‡æ¡£ä¸­çš„â€œå‚è€ƒç­”æ¡ˆâ€éƒ¨åˆ†ã€‚å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›'æ— 'ã€‚"
    try:
        response = client.chat.completions.create(
            model=AI_MODEL_NAME,
            messages=[{"role": "user", "content": prompt + "\n\n" + full_text[:80000]}],
            temperature=0.01,
            top_p=0.1,
            max_tokens=4000
        )
        return response.choices[0].message.content
    except: return ""

def process_single_chunk(args):
    """
    å•ä¸ªåˆ‡ç‰‡å¤„ç†é€»è¾‘
    args: (chunk, index, total_chunks, answer_key)
    """
    chunk, index, total, answer_key = args

    # å»ºè®®ä½¿ç”¨ f-string è¿›è¡Œæ‹¼æ¥
    prompt = f"""
    ### Role & Objective
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯•é¢˜æ•°æ®ç»“æ„åŒ–æå–å¼•æ“ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¯»å–éç»“æ„åŒ–çš„æ–‡æœ¬åˆ‡ç‰‡ï¼Œå°†å…¶è½¬æ¢ä¸ºä¸¥æ ¼çš„ JSON æ•°ç»„ã€‚
    ä½ çš„è¾“å‡ºå°†è¢«ä»£ç ç›´æ¥è§£æï¼Œå› æ­¤ä¸¥ç¦è¾“å‡ºä»»ä½• Markdown æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ã€å¼€åœºç™½æˆ–ç»“æŸè¯­ã€‚

    ### Context: Reference Answer Key
    ä»¥ä¸‹æ˜¯æœ¬æ–‡æ¡£çš„å…¨å±€å‚è€ƒç­”æ¡ˆï¼ˆä»…ä¾›åŒ¹é…ä½¿ç”¨ï¼‰ã€‚
    å½“ä½ åœ¨é¢˜ç›®æ–‡æœ¬ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆæ—¶ï¼Œè¯·æ ¹æ®ã€é¢˜å·ã€‘æˆ–ã€é¢˜ç›®å†…å®¹æ‘˜è¦ã€‘åœ¨æ­¤åº“ä¸­æŸ¥æ‰¾ã€‚
    --------------------------------------------------
    {answer_key[:2000]} ... (ç­”æ¡ˆåº“ç‰‡æ®µ)
    --------------------------------------------------

    ### Processing Rules (Strict Execution)

    1. **è¾¹ç•Œæˆªæ–­å¤„ç† (Anti-Truncation)**:
       - è¾“å…¥æ–‡æœ¬æ˜¯æ–‡æ¡£çš„ä¸€ä¸ªåˆ‡ç‰‡ï¼ˆChunkï¼‰ã€‚
       - **æ ¸å¿ƒè§„åˆ™**ï¼šå¦‚æœåˆ‡ç‰‡å¼€å¤´çš„ç¬¬ä¸€é¢˜ä¸å®Œæ•´ï¼ˆåªæœ‰é€‰é¡¹æ— é¢˜å¹²ï¼‰ï¼Œæˆ–è€…åˆ‡ç‰‡æœ«å°¾çš„æœ€åä¸€é¢˜ä¸å®Œæ•´ï¼ˆåªæœ‰é¢˜å¹²æ— é€‰é¡¹ï¼‰ï¼Œ**ç›´æ¥ä¸¢å¼ƒ**ã€‚åªæå–ä¸­é—´å®Œæ•´çš„é¢˜ç›®ã€‚

    2. **ç­”æ¡ˆåŒ¹é…é€»è¾‘ (Answer Matching)**:
       - **ä¼˜å…ˆçº§ 1**ï¼šé¢˜ç›®è‡ªå¸¦ç­”æ¡ˆï¼ˆä¾‹å¦‚é¢˜å¹²æ‹¬å·å†…ã€é¢˜å¹²æœ«å°¾ã€é€‰é¡¹ä¸‹æ–¹æ ‡æ³¨çš„â€œã€ç­”æ¡ˆã€‘â€ï¼‰ã€‚
       - **ä¼˜å…ˆçº§ 2**ï¼šå¦‚æœåœ¨æ–‡æœ¬ä¸­æ‰¾ä¸åˆ°ï¼Œè¯·å»ä¸Šé¢çš„ã€Reference Answer Keyã€‘ä¸­æŸ¥æ‰¾å¯¹åº”é¢˜å·çš„ç­”æ¡ˆã€‚
       - **ä¼˜å…ˆçº§ 3**ï¼šå¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œ`answer` å­—æ®µç•™ç©ºå­—ç¬¦ä¸²ã€‚

    3. **é¢˜å‹æ ‡å‡†åŒ– (Category Normalization)**:
       - æ ¹æ®é¢˜ç›®ç‰¹å¾ï¼ˆæ˜¯å¦æœ‰é€‰é¡¹ã€é€‰é¡¹æ•°é‡ã€æ˜¯å¦æœ‰â€œå¤šé€‰â€å­—æ ·ï¼‰è‡ªåŠ¨æ¨æ–­ `category` å’Œ `type`ã€‚
       - **å•é€‰é¢˜** (SINGLE_CHOICE): æœ‰ A,B,C,D é€‰é¡¹ï¼Œä¸”ç­”æ¡ˆåªæœ‰ä¸€ä¸ªã€‚
       - **å¤šé€‰é¢˜** (MULTI_CHOICE): æœ‰é€‰é¡¹ï¼Œä¸”ç­”æ¡ˆåŒ…å«å¤šä¸ªå­—æ¯ï¼Œæˆ–é¢˜å¹²æ ‡æ˜â€œå¤šé€‰/ä¸å®šé¡¹â€ã€‚
       - **åˆ¤æ–­é¢˜** (TRUE_FALSE): é€‰é¡¹ä¸º å¯¹/é”™ã€T/Fã€æ˜¯/å¦ã€‚
       - **å¡«ç©ºé¢˜** (FILL_BLANK): é¢˜å¹²ä¸­æœ‰ä¸‹åˆ’çº¿ `_` æˆ–æ‹¬å·ï¼Œä¸”æ— é€‰é¡¹ã€‚
       - **ç®€ç­”/è®¡ç®—/ç¼–ç¨‹** (ESSAY): æ— é€‰é¡¹ï¼Œéœ€è¦æ–‡å­—å›ç­”ã€‚

    4. **æ•°æ®æ¸…æ´—**:
       - ç§»é™¤é¢˜å¹²å¼€å¤´çš„é¢˜å·ï¼ˆå¦‚ "1. " æˆ– "(1)"ï¼‰ï¼Œå°†å…¶æ”¾å…¥ `number` å­—æ®µï¼ˆå¦‚æœæ— æ³•æå–åˆ™ç”±ä»£ç ç”Ÿæˆï¼‰ã€‚
       - ç§»é™¤é€‰é¡¹å¼€å¤´çš„æ ‡è¯†ç¬¦ï¼ˆå¦‚ "A."ï¼‰ï¼Œå°†å…¶æ”¾å…¥ `label` å­—æ®µã€‚

    ### Output Schema (JSON Array)
    è¯·è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

    [
      {{
        "category": "å•é€‰é¢˜",          // æ ‡å‡†åŒ–åˆ†ç±»ï¼šå•é€‰é¢˜/å¤šé€‰é¢˜/åˆ¤æ–­é¢˜/å¡«ç©ºé¢˜/ç®€ç­”é¢˜/åè¯è§£é‡Šé¢˜/è®¡ç®—é¢˜
        "type": "SINGLE_CHOICE",      // æšä¸¾ï¼šSINGLE_CHOICE / MULTI_CHOICE / TRUE_FALSE / FILL_BLANK / ESSAY
        "content": "é¢˜å¹²æ–‡æœ¬...",      // å¿…é¡»æ¸…æ´—æ‰å¼€å¤´çš„é¢˜å·
        "options": [                  // é€‰æ‹©é¢˜å¿…å¡«ï¼Œéé€‰æ‹©é¢˜ä¸ºç©ºæ•°ç»„ []
          {{"label": "A", "text": "é€‰é¡¹å†…å®¹"}},
          {{"label": "B", "text": "é€‰é¡¹å†…å®¹"}}
        ],
        "answer": "A",                // å¦‚æœæ˜¯å¤šé€‰åˆ™ä¸º "ABC"ï¼Œåˆ¤æ–­é¢˜ä¸º "æ­£ç¡®/é”™è¯¯"
        "analysis": "è§£æå†…å®¹"         // å¦‚æœæ–‡æœ¬ä¸­æœ‰ã€è§£æã€‘ï¼Œè¯·æå–ï¼›å¦åˆ™ç•™ç©º
      }}
    ]
    """
    
    # === æ™ºèƒ½é‡è¯•æœºåˆ¶ (Exponential Backoff) ===
    last_error = None
    for attempt in range(MAX_RETRIES):
        try:
            # åŠ¨æ€è°ƒæ•´æ¸©åº¦ï¼šå¦‚æœé‡è¯•ï¼Œç¨å¾®å¢åŠ ä¸€ç‚¹æ¸©åº¦é¿å…æ­»å¾ªç¯
            temp = 0.1 if attempt == 0 else 0.3
            
            response = client.chat.completions.create(
                model=AI_MODEL_NAME,
                messages=[{"role": "system", "content": prompt}, {"role": "user", "content": chunk}],
                temperature=0.01,
                top_p=0.1,
                max_tokens=4000
            )
            content = response.choices[0].message.content
            
            # æ¸…æ´—
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            content = content.strip()
            
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                # å°è¯•ä¿®å¤
                fixed = repair_json(content)
                return json.loads(fixed)
                
        except Exception as e:
            last_error = e
            # æŒ‡æ•°é€€é¿ï¼šç¬¬ä¸€æ¬¡ç­‰1sï¼Œç¬¬äºŒæ¬¡2sï¼Œç¬¬ä¸‰æ¬¡4s... åŠ ä¸ŠéšæœºæŠ–åŠ¨
            wait_time = (2 ** attempt) + random.uniform(0, 1)
            # print(f"âš ï¸ Chunk {index+1} å¤±è´¥ï¼Œ{wait_time:.1f}s åé‡è¯•... ({e})")
            time.sleep(wait_time)
    
    # å¦‚æœé‡è¯• 5 æ¬¡éƒ½å¤±è´¥
    print(f"âŒ Chunk {index+1} å½»åº•å¤±è´¥ï¼Œå·²è·³è¿‡ã€‚é”™è¯¯: {last_error}")
    return []

def main():
    start_time = time.time()
    
    if not os.path.exists(INPUT_DIR): os.makedirs(INPUT_DIR)
    docx_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(".docx")]
    
    if not docx_files:
        print("âŒ input ç›®å½•ä¸ºç©ºã€‚")
        return
    
    all_questions = []
    seen_hashes = set()
    
    print(f"ğŸš€ æé€Ÿæ¨¡å¼å¯åŠ¨ | å¹¶å‘: {MAX_WORKERS} | æ–‡æ¡£æ•°: {len(docx_files)}")

    for filename in docx_files:
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        raw_text = read_docx(os.path.join(INPUT_DIR, filename))
        if not raw_text: continue

        # 1. æå–ç­”æ¡ˆ
        global_answers = extract_global_answers(raw_text)

        # 2. åˆ‡ç‰‡
        chunks = get_chunks(raw_text, CHUNK_SIZE, OVERLAP)
        
        # 3. å¹¶å‘å¤„ç† (å¸¦è¿›åº¦æ¡)
        # å‡†å¤‡å‚æ•°
        tasks_args = [(chunk, i, len(chunks), global_answers) for i, chunk in enumerate(chunks)]
        
        chunk_added_count = 0
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
            results = list(tqdm(executor.map(process_single_chunk, tasks_args), total=len(chunks), unit="åˆ‡ç‰‡"))
            
            for items in results:
                if items:
                    for item in items:
                        fp = generate_fingerprint(item)
                        if fp in seen_hashes: continue
                        seen_hashes.add(fp)
                        
                        item['category'] = normalize_category(item.get('category', 'ç»¼åˆé¢˜'))
                        item['id'] = str(uuid.uuid4())
                        item['number'] = len(all_questions) + 1
                        item['chapter'] = filename.replace(".docx", "")
                        all_questions.append(item)
                        chunk_added_count += 1
                        
        print(f"   âœ… æå–å®Œæˆ: {chunk_added_count} é“é¢˜")

    # ä¿å­˜
    final_json = {
        "version": "Turbo-V7",
        "total_count": len(all_questions),
        "data": all_questions
    }
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    out_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(final_json, f, ensure_ascii=False, indent=2)

    duration = time.time() - start_time
    msg = f"å¤„ç†å®Œæˆï¼\nè€—æ—¶: {duration:.1f}s\næ€»é¢˜æ•°: {len(all_questions)}\nå¹¶å‘: {MAX_WORKERS}"
    print(f"\nâœ¨ {msg}")
    send_notification("âœ… é¢˜åº“è½¬æ¢(æé€Ÿç‰ˆ)", msg.replace('\n', '<br>'))

if __name__ == "__main__":
    main()