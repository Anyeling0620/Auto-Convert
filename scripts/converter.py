import json
import os
import uuid
import hashlib
import re
from docx import Document
from zhipuai import ZhipuAI

# === é…ç½®åŒºåŸŸ ===
INPUT_DIR = "input"
OUTPUT_DIR = "output"
OUTPUT_FILE = "questions_full.json"
ZHIPU_API_KEY = os.getenv("ZHIPU_API_KEY")

client = ZhipuAI(api_key=ZHIPU_API_KEY)

# === æ ‡å‡†åˆ†ç±»ç™½åå• (è¿™æ˜¯ä½ å¸Œæœ›APPé‡Œå‡ºç°çš„æ ‡å‡†å«æ³•) ===
STANDARD_CATEGORIES = {
    "å•é€‰é¢˜", "å¤šé€‰é¢˜", "åˆ¤æ–­é¢˜", "å¡«ç©ºé¢˜", "ç®€ç­”é¢˜", 
    "åè¯è§£é‡Šé¢˜", "æ¡ˆä¾‹åˆ†æé¢˜", "è®¡ç®—é¢˜", "è¯æ˜é¢˜", "é…ä¼é¢˜"
}

def read_docx(file_path):
    if not os.path.exists(file_path): return ""
    doc = Document(file_path)
    # è¿‡æ»¤ç©ºè¡Œï¼Œä¿ç•™æ®µè½ç»“æ„ï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥
    return "\n".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])

def get_chunks(text, chunk_size=3000, overlap=500):
    """æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼Œä¿è¯é•¿é¢˜ç›®ä¸è¢«æˆªæ–­"""
    chunks = []
    start = 0
    total_len = len(text)
    while start < total_len:
        end = min(start + chunk_size, total_len)
        chunks.append(text[start:end])
        if end == total_len: break
        start = end - overlap
    return chunks

def generate_fingerprint(q_obj):
    """ç”ŸæˆæŒ‡çº¹ç”¨äºå»é‡"""
    raw = q_obj.get("content", "") + str(q_obj.get("options", ""))
    return hashlib.md5(raw.encode('utf-8')).hexdigest()

def normalize_category(raw_cat):
    """
    ã€æ ¸å¿ƒé€»è¾‘ã€‘å¼ºåˆ¶å½’ä¸€åŒ–åˆ†ç±»åç§°
    ä¸ç®¡AIè¾“å‡ºä»€ä¹ˆï¼Œåªè¦åŒ…å«å…³é”®å­—ï¼Œå°±å¼ºåˆ¶æ˜ å°„åˆ°æ ‡å‡†è¯ã€‚
    """
    if not raw_cat: return "ç»¼åˆé¢˜"
    
    cat = raw_cat.strip()
    
    # 1. å…³é”®å­—å¼ºåˆ¶æ˜ å°„ (ä¼˜å…ˆçº§ä»é«˜åˆ°ä½)
    if "å¤šé€‰" in cat or "ä¸å®šé¡¹" in cat: return "å¤šé€‰é¢˜"
    if "å•é€‰" in cat: return "å•é€‰é¢˜"
    if "åˆ¤æ–­" in cat or "æ˜¯é" in cat: return "åˆ¤æ–­é¢˜"
    if "å¡«ç©º" in cat: return "å¡«ç©ºé¢˜"
    if "åè¯" in cat: return "åè¯è§£é‡Šé¢˜"
    if "è®¡ç®—" in cat: return "è®¡ç®—é¢˜"
    if "è¯æ˜" in cat: return "è¯æ˜é¢˜"
    if "æ¡ˆä¾‹" in cat or "ç—…ä¾‹" in cat: return "æ¡ˆä¾‹åˆ†æé¢˜"
    if "é…ä¼" in cat or "è¿çº¿" in cat: return "é…ä¼é¢˜"
    if "ç®€ç­”" in cat or "é—®ç­”" in cat or "è®ºè¿°" in cat: return "ç®€ç­”é¢˜"
    
    # 2. å¦‚æœæ²¡å‘½ä¸­æ ‡å‡†è¯ï¼Œä½†å·²ç»åœ¨ç™½åå•é‡Œï¼Œç›´æ¥è¿”å›
    if cat in STANDARD_CATEGORIES:
        return cat
        
    # 3. å…œåº•è§„åˆ™ï¼šå¦‚æœAIåˆ›é€ äº†æ–°è¯ï¼ˆæ¯”å¦‚"ä½œå›¾"ï¼‰ï¼Œå¼ºåˆ¶åŠ ä¸Šåç¼€"é¢˜"
    if not cat.endswith("é¢˜"):
        return cat + "é¢˜"
        
    return cat

def call_glm4(text_chunk):
    # Prompt ä¼˜åŒ–ï¼šå¼ºè°ƒé€šç”¨æ€§å’Œæ ‡å‡†å‘½å
    prompt = """
    ä½ æ˜¯ä¸€ä¸ªé€šç”¨è¯•é¢˜ç»“æ„åŒ–åŠ©æ‰‹ã€‚è¯·è¯†åˆ«è¾“å…¥æ–‡æœ¬ä¸­çš„é¢˜ç›®ï¼Œå¹¶è½¬æ¢ä¸º JSON æ•°ç»„ã€‚
    
    ã€å¤„ç†åŸåˆ™ã€‘
    1. **é€šç”¨æ€§**ï¼šä¸è¦é¢„è®¾å­¦ç§‘ï¼Œæ ¹æ®é¢˜ç›®å†…å®¹å’Œé€‰é¡¹ç‰¹å¾è‡ªåŠ¨æ¨æ–­ã€‚
    2. **æ ‡å‡†å‘½å**ï¼šcategory å­—æ®µè¯·ä¼˜å…ˆä½¿ç”¨ä»¥ä¸‹æ ‡å‡†åç§°ï¼š
       - "å•é€‰é¢˜", "å¤šé€‰é¢˜", "åˆ¤æ–­é¢˜", "å¡«ç©ºé¢˜", "ç®€ç­”é¢˜", "åè¯è§£é‡Šé¢˜", "è®¡ç®—é¢˜"
       - åªæœ‰å½“é¢˜ç›®å®Œå…¨ä¸ç¬¦åˆä¸Šè¿°ç±»å‹æ—¶ï¼Œæ‰å¯ä»¥ä½¿ç”¨å…¶ä»–åç§°ï¼ˆå¦‚"ä½œå›¾é¢˜"ï¼‰ã€‚
    3. **å®Œæ•´æ€§**ï¼šå¿½ç•¥åˆ‡ç‰‡å¼€å¤´å’Œç»“å°¾çš„æ®‹ç¼ºé¢˜ç›®ã€‚

    ã€è¾“å‡ºæ ¼å¼ã€‘
    Strict JSON Array ONLY:
    [
      {
        "category": "String (ä¼˜å…ˆæ ‡å‡†è¯)",
        "type": "SINGLE_CHOICE | MULTI_CHOICE | TRUE_FALSE | FILL_BLANK | ESSAY",
        "content": "é¢˜å¹²å†…å®¹",
        "options": [{"label":"A", "text":"..."}], 
        "answer": "å‚è€ƒç­”æ¡ˆ",
        "analysis": "è§£æ (æ— åˆ™ç•™ç©º)"
      }
    ]
    """
    
    try:
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": text_chunk}
            ],
            temperature=0.1, # ä½æ¸©ï¼Œå‡å°‘AIèƒ¡ç¼–ä¹±é€ 
            top_p=0.7
        )
        content = response.choices[0].message.content
        # æ¸…æ´—å¯èƒ½å­˜åœ¨çš„ Markdown æ ‡è®°
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
            
        return json.loads(content.strip())
    except Exception as e:
        print(f"âš ï¸ Chunk parse warning: {e}")
        return []

def main():
    # æ‰«æ input æ–‡ä»¶å¤¹
    docx_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(".docx")]
    if not docx_files:
        print("âŒ No .docx files found in input/ directory.")
        return
    
    # åªå¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œæˆ–è€…ä½ å¯ä»¥æ”¹æˆå¾ªç¯å¤„ç†æ‰€æœ‰
    target_file = os.path.join(INPUT_DIR, docx_files[0])
    print(f"ğŸš€ Processing: {target_file}")
    
    raw_text = read_docx(target_file)
    if not raw_text:
        print("âŒ File is empty or could not be read.")
        return

    chunks = get_chunks(raw_text)
    print(f"ğŸ“‚ Split document into {len(chunks)} chunks.")
    
    all_questions = []
    seen_hashes = set()
    
    for i, chunk in enumerate(chunks):
        print(f"âš¡ Analyzing chunk {i+1}/{len(chunks)}...")
        items = call_glm4(chunk)
        
        new_count = 0
        for item in items:
            # ç”ŸæˆæŒ‡çº¹
            fp = generate_fingerprint(item)
            if fp in seen_hashes: continue # è·³è¿‡é‡å¤
            
            seen_hashes.add(fp)
            
            # === å…³é”®æ­¥éª¤ï¼šå¼ºåˆ¶å½’ä¸€åŒ– ===
            # è¿™é‡Œè°ƒç”¨æ¸…æ´—å‡½æ•°ï¼Œç¡®ä¿ "åˆ¤æ–­" -> "åˆ¤æ–­é¢˜"
            raw_cat = item.get('category', 'ç»¼åˆé¢˜')
            item['category'] = normalize_category(raw_cat)
            
            # è¡¥å…¨å…¶ä»–å­—æ®µ
            item['id'] = str(uuid.uuid4())
            item['number'] = len(all_questions) + 1
            if 'chapter' not in item: 
                # è¿™é‡Œå¯ä»¥ç®€å•å†™æ­»ï¼Œæˆ–è€…è®© AI æå–ã€‚ä¸ºäº†é€šç”¨æ€§ï¼Œå†™ "å¯¼å…¥é¢˜åº“" æ¯”è¾ƒå®‰å…¨
                item['chapter'] = "å¯¼å…¥é¢˜åº“" 
            
            all_questions.append(item)
            new_count += 1
            
        print(f"   -> Added {new_count} questions.")

    # ç»“æœä¿å­˜
    final_json = {
        "version": "Universal-V1",
        "source": docx_files[0],
        "total_count": len(all_questions),
        "data": all_questions
    }
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    out_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(final_json, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… Conversion Complete! Saved to: {out_path}")

if __name__ == "__main__":
    main()