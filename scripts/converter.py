import json
import os
import uuid
import hashlib
import time
import requests
import random
import threading
from docx import Document
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
INPUT_DIR = "input"
OUTPUT_DIR = "output"
OUTPUT_FILE = "questions_full.json"

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
PUSHPLUS_TOKEN = os.getenv("PUSHPLUS_TOKEN")

# 1. æ›¿æ¢ Base URL (è¿™æ˜¯ç¡…åŸºæµåŠ¨çš„ API åœ°å€)
AI_BASE_URL = "https://api.siliconflow.cn/v1"

# 2. æ›¿æ¢æ¨¡å‹åç§° (æ³¨æ„ï¼šç¡…åŸºæµåŠ¨çš„æ¨¡å‹åé€šå¸¸å¸¦æœ‰ deepseek-ai å‰ç¼€)
# å…·ä½“åç§°è¯·å»ç¡…åŸºæµåŠ¨åå°ç¡®è®¤ï¼Œé€šå¸¸æ˜¯ "deepseek-ai/DeepSeek-V3"
AI_MODEL_NAME = "deepseek-ai/DeepSeek-V3"

API_TIMEOUT = 120  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 120 ç§’

# ã€ç¨³å®šæ¨¡å¼é…ç½®ã€‘
# å¹¶å‘æ•°ï¼šé™å› 8ï¼Œä¿è¯ä¸æ’å¢™
MAX_WORKERS = 8
CHUNK_SIZE = 2000
OVERLAP = 200
MAX_RETRIES = 5
# å‘å°„é—´éš”ï¼šæ¯ 0.5 ç§’å‘å°„ä¸€ä¸ªè¯·æ±‚ï¼Œå¹³æ»‘æµé‡
REQUEST_INTERVAL = 0.5

# å…¨å±€å†·å´é”ï¼šå½“é‡åˆ° 429 æ—¶ï¼Œæ‰€æœ‰çº¿ç¨‹æš‚ç¼“å‘é€
GLOBAL_COOLDOWN_EVENT = threading.Event()
GLOBAL_COOLDOWN_EVENT.set()  # åˆå§‹çŠ¶æ€ä¸ºç»¿ç¯

# ===========================================

if not DEEPSEEK_API_KEY:
    print("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ° DEEPSEEK_API_KEY")
    exit(1)

client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=AI_BASE_URL)

STANDARD_CATEGORIES = {
    "å•é€‰é¢˜", "å¤šé€‰é¢˜", "åˆ¤æ–­é¢˜", "å¡«ç©ºé¢˜",
    "åè¯è§£é‡Šé¢˜", "ç®€ç­”é¢˜", "è®ºè¿°é¢˜",
    "è®¡ç®—é¢˜", "è¯æ˜é¢˜", "åº”ç”¨é¢˜", "ç¼–ç¨‹é¢˜",
    "é…ä¼é¢˜", "æ¡ˆä¾‹åˆ†æé¢˜", "ç»¼åˆé¢˜"
}


def send_notification(title, content):
    if not PUSHPLUS_TOKEN: return
    try:
        requests.post("http://www.pushplus.plus/send", json={
            "token": PUSHPLUS_TOKEN, "title": title, "content": content, "template": "html"
        }, timeout=5)
    except:
        pass


def read_docx(file_path):
    if not os.path.exists(file_path): return ""
    try:
        doc = Document(file_path)
        return "\n".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])
    except:
        return ""


def get_chunks(text, chunk_size, overlap):
    chunks = []
    start = 0
    total_len = len(text)
    while start < total_len:
        end = min(start + chunk_size, total_len)
        chunks.append(text[start:end])
        if end == total_len: break
        start = end - overlap
    return chunks


def generate_fingerprint(q_obj):
    raw = q_obj.get("content", "") + str(q_obj.get("options", ""))
    return hashlib.md5(raw.encode('utf-8')).hexdigest()


def normalize_category(raw_cat):
    if not raw_cat: return "ç»¼åˆé¢˜"
    cat = raw_cat.strip()
    if "å¤šé€‰" in cat or "ä¸å®šé¡¹" in cat: return "å¤šé€‰é¢˜"
    if "å•é€‰" in cat or "A1" in cat or "A2" in cat: return "å•é€‰é¢˜"
    if "åˆ¤æ–­" in cat or "æ˜¯é" in cat: return "åˆ¤æ–­é¢˜"
    if "å¡«ç©º" in cat: return "å¡«ç©ºé¢˜"
    if "é…ä¼" in cat or "è¿çº¿" in cat or "B1" in cat: return "é…ä¼é¢˜"
    if "åè¯" in cat: return "åè¯è§£é‡Šé¢˜"
    if "ç®€ç­”" in cat or "é—®ç­”" in cat: return "ç®€ç­”é¢˜"
    if "è®¡ç®—" in cat: return "è®¡ç®—é¢˜"
    if "ç¼–ç¨‹" in cat or "ä»£ç " in cat: return "ç¼–ç¨‹é¢˜"
    if "åº”ç”¨" in cat: return "åº”ç”¨é¢˜"
    if "è¯æ˜" in cat: return "è¯æ˜é¢˜"
    if "æ¡ˆä¾‹" in cat or "ç—…ä¾‹" in cat: return "æ¡ˆä¾‹åˆ†æé¢˜"
    if cat in STANDARD_CATEGORIES: return cat
    if not cat.endswith("é¢˜"): return cat + "é¢˜"
    return cat


def repair_json(json_str):
    json_str = json_str.strip()
    if "```json" in json_str:
        json_str = json_str.split("```json")[1].split("```")[0]
    elif "```" in json_str:
        json_str = json_str.split("```")[1].split("```")[0]
    json_str = json_str.strip()
    if not json_str.endswith("]"):
        last_brace = json_str.rfind("}")
        if last_brace != -1:
            json_str = json_str[:last_brace + 1] + "]"
    return json_str


def extract_global_answers(full_text):
    print("   ğŸ” [Step 1] DeepSeek æ­£åœ¨å…¨æ–‡æ‰«æå‚è€ƒç­”æ¡ˆ...")
    # å®‰å…¨æˆªå–ï¼Œé˜²æ­¢è¶…é•¿
    safe_text = full_text[:100000]
    prompt = """
    ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æå¸ˆã€‚è¯·æå–æ–‡æ¡£ä¸­çš„â€œå‚è€ƒç­”æ¡ˆâ€éƒ¨åˆ†ã€‚
    è¦æ±‚ï¼šåªæå–ç­”æ¡ˆæ–‡æœ¬ï¼ˆå¦‚ 1.A 2.Bï¼‰ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆ—è¡¨ã€‚
    """
    try:
        response = client.chat.completions.create(
            model=AI_MODEL_NAME,
            messages=[{"role": "system", "content": prompt}, {"role": "user", "content": safe_text}],
            temperature=0.1,
            timeout=120
        )
        ans = response.choices[0].message.content
        print(f"   âœ… å‚è€ƒç­”æ¡ˆåº“æ„å»ºå®Œæˆ (é•¿åº¦: {len(ans)} å­—ç¬¦)")
        return ans
    except Exception as e:
        print(f"   âš ï¸ ç­”æ¡ˆæå–å¤±è´¥: {e}")
        return ""


def trigger_global_cooldown():
    """è§¦å‘å…¨å±€å†·å´ï¼šå¦‚æœæœ‰ä¸€ä¸ªçº¿ç¨‹è¢«é™æµï¼Œå¤§å®¶ä¸€èµ·åœä¸€ä¼š"""
    if GLOBAL_COOLDOWN_EVENT.is_set():
        # print("   â„ï¸ æ£€æµ‹åˆ°é™æµï¼Œå…¨å±€æš‚åœ 5 ç§’...")
        GLOBAL_COOLDOWN_EVENT.clear()  # çº¢ç¯
        time.sleep(5)
        GLOBAL_COOLDOWN_EVENT.set()  # ç»¿ç¯


def process_single_chunk(args):
    chunk, index, total, answer_key = args

    # ==================================================================================
    # âš¡ ç»ˆæä¸¥è°¨ç‰ˆ Prompt (ä¸­æ–‡å·¥ä¸šçº§)
    # ==================================================================================
    prompt = f"""
    [ç³»ç»Ÿè§’è‰²è®¾å®š]
    ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„â€œè¯•é¢˜æ•°æ®ç»“æ„åŒ–æå–å¼•æ“â€ã€‚ä½ **ä¸æ˜¯**èŠå¤©åŠ©æ‰‹ã€‚
    ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯å°†è¾“å…¥çš„æ–‡æœ¬åˆ‡ç‰‡è§£æä¸ºåˆæ³•çš„ JSON æ•°ç»„ã€‚

    [å…¨å±€ä¸Šä¸‹æ–‡ï¼šå‚è€ƒç­”æ¡ˆåº“]
    -----------------------------------------------------------------------
    {answer_key[:15000]} ... (è‹¥è¿‡é•¿å·²æˆªæ–­)
    -----------------------------------------------------------------------

    [ä¸¥æ ¼æ‰§è¡Œå®ˆåˆ™]

    1. **è¾¹ç•Œæˆªæ–­å¤„ç† (è‡³å…³é‡è¦)**
       - è¾“å…¥æ–‡æœ¬æ˜¯é•¿æ–‡æ¡£çš„ä¸€ä¸ªåˆ‡ç‰‡ã€‚
       - **ç›´æ¥ä¸¢å¼ƒ**åˆ‡ç‰‡å¼€å¤´æˆ–ç»“å°¾å¤„ä¸å®Œæ•´çš„æ®‹ç¼ºå¥å­ï¼ˆä¾‹å¦‚åªæœ‰é€‰é¡¹æ²¡æœ‰é¢˜å¹²ï¼Œæˆ–åªæœ‰é¢˜å¹²æ²¡æœ‰é€‰é¡¹ï¼‰ã€‚
       - åªæå–ä¸­é—´å®Œæ•´çš„é¢˜ç›®ã€‚

    2. **ç­”æ¡ˆåŒ¹é…é€»è¾‘ (ä¼˜å…ˆçº§é¡ºåº)**
       - **ä¼˜å…ˆçº§ 1 (è‡ªå¸¦ç­”æ¡ˆ)**ï¼šä¼˜å…ˆæå–é¢˜ç›®æ–‡æœ¬ä¸­è‡ªå¸¦çš„ç­”æ¡ˆï¼ˆä¾‹å¦‚æ‹¬å·å†…çš„ç­”æ¡ˆã€é¢˜å¹²æœ«å°¾çš„ç­”æ¡ˆã€é€‰é¡¹ä¸‹æ–¹çš„â€œã€ç­”æ¡ˆã€‘â€ï¼‰ã€‚
       - **ä¼˜å…ˆçº§ 2 (æŸ¥å…¨å±€åº“)**ï¼šæå–ã€é¢˜å·ã€‘ï¼ˆå¦‚ "53."ï¼‰ï¼Œå»ä¸Šæ–¹çš„ [å…¨å±€ä¸Šä¸‹æ–‡ï¼šå‚è€ƒç­”æ¡ˆåº“] ä¸­æŸ¥æ‰¾å¯¹åº”ç­”æ¡ˆã€‚
       - **ä¼˜å…ˆçº§ 3 (ç•™ç©º)**ï¼šå¦‚æœä»¥ä¸Šä¸¤è€…éƒ½æ‰¾ä¸åˆ°ï¼Œ`answer` å­—æ®µå¿…é¡»ç•™ç©ºå­—ç¬¦ä¸² ""ã€‚**ä¸¥ç¦ççŒœã€‚**

    3. **æ•°æ®æ¸…æ´—è§„åˆ™**
       - **å†…å®¹æ¸…æ´—**ï¼šç§»é™¤é¢˜å¹²å¼€å¤´çš„é¢˜å·ï¼ˆä¾‹å¦‚å°† "1. ä»€ä¹ˆæ˜¯..." æ¸…æ´—ä¸º "ä»€ä¹ˆæ˜¯..."ï¼‰ã€‚
       - **é€‰é¡¹æ¸…æ´—**ï¼šç§»é™¤é€‰é¡¹å¼€å¤´çš„æ ‡ç­¾ï¼ˆä¾‹å¦‚å°† "A. è‹¹æœ" æ¸…æ´—ä¸º "è‹¹æœ"ï¼‰ï¼Œæ ‡ç­¾æ”¾å…¥ `label` å­—æ®µã€‚
       - **ç±»å‹æ¨æ–­ (Type Inference)**ï¼š
         - 4ä¸ªé€‰é¡¹ + 1ä¸ªç­”æ¡ˆ = "SINGLE_CHOICE"
         - é€‰é¡¹æ˜¯ å¯¹/é”™ æˆ– T/F = "TRUE_FALSE"
         - å¤šä¸ªç­”æ¡ˆ (å¦‚ "ABC") æˆ–åŒ…å«å…³é”®å­— "å¤šé€‰/ä¸å®šé¡¹" = "MULTI_CHOICE"
         - æ— é€‰é¡¹ + ä¸‹åˆ’çº¿ "_" æˆ– "()" = "FILL_BLANK"
         - æ— é€‰é¡¹ + é—®ç­”/ç®€è¿°/ä»£ç /è®¡ç®— = "ESSAY"

    4. **é¢˜å‹å½’ä¸€åŒ– (ä¸¥æ ¼ç™½åå•)**
       - `category` å­—æ®µåªèƒ½æ˜¯ä»¥ä¸‹å€¼ä¹‹ä¸€ï¼š
         "å•é€‰é¢˜", "å¤šé€‰é¢˜", "åˆ¤æ–­é¢˜", "å¡«ç©ºé¢˜", "åè¯è§£é‡Šé¢˜", "ç®€ç­”é¢˜", "è®¡ç®—é¢˜", "æ¡ˆä¾‹åˆ†æé¢˜", "é…ä¼é¢˜", "ç¼–ç¨‹é¢˜"ã€‚
       - å¦‚æœæ‹¿ä¸å‡†ï¼Œå½’ç±»ä¸º "ç»¼åˆé¢˜"ã€‚

    [è¾“å‡ºæ ¼å¼è§„èŒƒ]
    - è¾“å‡ºå¿…é¡»æ˜¯åˆæ³•çš„ JSON Arrayã€‚
    - **ä¸¥ç¦**è¾“å‡º Markdown ä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ã€‚
    - **ä¸¥ç¦**åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–å¼€åœºç™½ã€‚
    - å­—æ®µ `options` å¿…é¡»æ˜¯å¯¹è±¡æ•°ç»„ï¼š{{"label": "A", "text": "..."}}ã€‚

    [å¾…å¤„ç†æ–‡æœ¬åˆ‡ç‰‡]
    {chunk}
    """
    # ==================================================================================

    for attempt in range(MAX_RETRIES):
        try:
            # åŠ¨æ€æ¸©åº¦æ§åˆ¶ï¼šåˆæ¬¡å°è¯•ç»å¯¹ç†æ€§ï¼Œé‡è¯•æ—¶ç¨å¾®ç»™ç‚¹çµæ´»æ€§
            current_temp = 0.0 if attempt < 2 else 0.2

            response = client.chat.completions.create(
                model=AI_MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=current_temp,
                max_tokens=4000,
                timeout=API_TIMEOUT
            )
            content = response.choices[0].message.content

            # æ·±åº¦æ¸…æ´—ï¼šé˜²æ­¢ AI è™½ç„¶å¬è¯ä½†è¿˜æ˜¯å¿ä¸ä½åŠ äº† ```json
            content = repair_json(content)

            try:
                parsed_json = json.loads(content)
                if isinstance(parsed_json, list):
                    return parsed_json
                elif isinstance(parsed_json, dict):
                    return [parsed_json]
                else:
                    return []
            except json.JSONDecodeError:
                if attempt == MAX_RETRIES - 1:
                    print(f"      âŒ Chunk {index + 1} JSON è§£æå½»åº•å¤±è´¥: {content[:50]}...")
                continue

        except Exception as e:
            # é”™è¯¯å¤„ç†ä¿æŒä¸å˜...
            wait_time = (2 ** attempt) + random.uniform(0, 1)
            # ä»…åœ¨å¤šæ¬¡é‡è¯•åæ‰“å°æ—¥å¿—ï¼Œä¿æŒæ§åˆ¶å°æ¸…çˆ½
            if attempt > 2:
                print(f"      âš ï¸ Chunk {index + 1} ç¬¬ {attempt + 1} æ¬¡é‡è¯•: {e}")
            time.sleep(wait_time)

    return []

def main():
    start_time = time.time()

    if not os.path.exists(INPUT_DIR): os.makedirs(INPUT_DIR)
    docx_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(".docx")]

    if not docx_files:
        print("âŒ input ç›®å½•ä¸ºç©ºã€‚")
        return

    all_questions = []
    seen_hashes = set()

    print(f"ğŸš€ DeepSeek ç¨³å®šæ¨¡å¼ | å¹¶å‘: {MAX_WORKERS} | èŠ‚æµé—´éš”: {REQUEST_INTERVAL}s")

    for filename in docx_files:
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        raw_text = read_docx(os.path.join(INPUT_DIR, filename))
        if not raw_text: continue

        global_answers = extract_global_answers(raw_text)
        chunks = get_chunks(raw_text, CHUNK_SIZE, OVERLAP)

        tasks_args = [(chunk, i, len(chunks), global_answers) for i, chunk in enumerate(chunks)]
        chunk_added = 0

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æ‰‹åŠ¨æäº¤ä»»åŠ¡ï¼Œæ§åˆ¶å‘å°„é¢‘ç‡
            futures = []
            for arg in tasks_args:
                futures.append(executor.submit(process_single_chunk, arg))
                # ã€æ ¸å¿ƒã€‘ï¼šæ¯å‘å°„ä¸€é¢—å­å¼¹ï¼Œåœé¡¿ä¸€ä¸‹ï¼Œé˜²æ­¢ç¬é—´å‡»ç©¿ API é™åˆ¶
                time.sleep(REQUEST_INTERVAL)

            # ä½¿ç”¨ tqdm ç›‘æ§ç»“æœ
            for future in tqdm(as_completed(futures), total=len(chunks), unit="åˆ‡ç‰‡"):
                items = future.result()
                if items:
                    for item in items:
                        fp = generate_fingerprint(item)
                        if fp in seen_hashes: continue
                        seen_hashes.add(fp)
                        item['category'] = normalize_category(item.get('category', 'ç»¼åˆé¢˜'))
                        item['id'] = str(uuid.uuid4())
                        item['number'] = len(all_questions) + 1
                        item['chapter'] = filename.replace(".docx", "")
                        all_questions.append(item)
                        chunk_added += 1

        print(f"   âœ… æå–å®Œæˆ: {chunk_added} é“é¢˜")

    final_json = {
        "version": "DeepSeek-Stable",
        "total_count": len(all_questions),
        "data": all_questions
    }

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    out_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(final_json, f, ensure_ascii=False, indent=2)

    duration = time.time() - start_time
    msg = f"DeepSeek å¤„ç†å®Œæˆï¼\nè€—æ—¶: {duration:.1f}s\né¢˜ç›®: {len(all_questions)}"
    print(f"\nâœ¨ {msg}")
    send_notification("âœ… é¢˜åº“è½¬æ¢æˆåŠŸ", msg.replace('\n', '<br>'))


if __name__ == "__main__":
    main()