import json
import os
import uuid
import hashlib
import time
import requests
import random
from docx import Document
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
INPUT_DIR = "input"
OUTPUT_DIR = "output"
OUTPUT_FILE = "questions_full.json"

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
PUSHPLUS_TOKEN = os.getenv("PUSHPLUS_TOKEN")

AI_BASE_URL = "https://api.deepseek.com"
AI_MODEL_NAME = "deepseek-chat"

# DeepSeek é€Ÿç‡é™åˆ¶è¾ƒä¸ºä¸¥æ ¼ï¼Œå»ºè®® 5-10
MAX_WORKERS = 8
# åˆ‡ç‰‡å¤§å°ï¼š2000 å­—ç¬¦
CHUNK_SIZE = 2000
OVERLAP = 200
MAX_RETRIES = 5

# ===========================================

if not DEEPSEEK_API_KEY:
    print("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ° DEEPSEEK_API_KEY")
    exit(1)

client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=AI_BASE_URL)

STANDARD_CATEGORIES = {
    "å•é€‰é¢˜", "å¤šé€‰é¢˜", "åˆ¤æ–­é¢˜", "å¡«ç©ºé¢˜",
    "åè¯è§£é‡Šé¢˜", "ç®€ç­”é¢˜", "è®ºè¿°é¢˜",
    "è®¡ç®—é¢˜", "è¯æ˜é¢˜", "åº”ç”¨é¢˜", "ç¼–ç¨‹é¢˜",
    "é…ä¼é¢˜", "æ¡ˆä¾‹åˆ†æé¢˜", "ç»¼åˆé¢˜"
}


def send_notification(title, content):
    if not PUSHPLUS_TOKEN: return
    try:
        requests.post("http://www.pushplus.plus/send", json={
            "token": PUSHPLUS_TOKEN, "title": title, "content": content, "template": "html"
        }, timeout=5)
    except:
        pass


def read_docx(file_path):
    if not os.path.exists(file_path): return ""
    try:
        doc = Document(file_path)
        return "\n".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])
    except:
        return ""


def get_chunks(text, chunk_size, overlap):
    chunks = []
    start = 0
    total_len = len(text)
    while start < total_len:
        end = min(start + chunk_size, total_len)
        chunks.append(text[start:end])
        if end == total_len: break
        start = end - overlap
    return chunks


def generate_fingerprint(q_obj):
    raw = q_obj.get("content", "") + str(q_obj.get("options", ""))
    return hashlib.md5(raw.encode('utf-8')).hexdigest()


def normalize_category(raw_cat):
    if not raw_cat: return "ç»¼åˆé¢˜"
    cat = raw_cat.strip()
    if "å¤šé€‰" in cat or "ä¸å®šé¡¹" in cat: return "å¤šé€‰é¢˜"
    if "å•é€‰" in cat or "A1" in cat or "A2" in cat: return "å•é€‰é¢˜"
    if "åˆ¤æ–­" in cat or "æ˜¯é" in cat: return "åˆ¤æ–­é¢˜"
    if "å¡«ç©º" in cat: return "å¡«ç©ºé¢˜"
    if "é…ä¼" in cat or "è¿çº¿" in cat or "B1" in cat: return "é…ä¼é¢˜"
    if "åè¯" in cat: return "åè¯è§£é‡Šé¢˜"
    if "ç®€ç­”" in cat or "é—®ç­”" in cat: return "ç®€ç­”é¢˜"
    if "è®¡ç®—" in cat: return "è®¡ç®—é¢˜"
    if "ç¼–ç¨‹" in cat or "ä»£ç " in cat: return "ç¼–ç¨‹é¢˜"
    if "åº”ç”¨" in cat: return "åº”ç”¨é¢˜"
    if "è¯æ˜" in cat: return "è¯æ˜é¢˜"
    if "æ¡ˆä¾‹" in cat or "ç—…ä¾‹" in cat: return "æ¡ˆä¾‹åˆ†æé¢˜"

    if cat in STANDARD_CATEGORIES: return cat
    if not cat.endswith("é¢˜"): return cat + "é¢˜"
    return cat


def repair_json(json_str):
    json_str = json_str.strip()
    if not json_str.endswith("]"):
        last_brace = json_str.rfind("}")
        if last_brace != -1:
            json_str = json_str[:last_brace + 1] + "]"
    return json_str


def extract_global_answers(full_text):
    """
    ã€å…³é”®ä¿®æ”¹ã€‘è¯»å–å…¨æ–‡ï¼Œæå–åˆ†æ•£çš„ç­”æ¡ˆ
    """
    print("   ğŸ” [Step 1] DeepSeek æ­£åœ¨å…¨æ–‡æ‰«æå‚è€ƒç­”æ¡ˆ (æ­¤è¿‡ç¨‹å¯èƒ½è¾ƒæ…¢)...")

    # DeepSeek æ”¯æŒ 64K contextï¼Œè¿™é‡Œæˆªå–å‰ 100,000 å­—ç¬¦ (çº¦5ä¸‡æ±‰å­—)ï¼Œè¦†ç›–ç»å¤§å¤šæ•°æ–‡æ¡£
    # å¦‚æœæ–‡æ¡£ç‰¹åˆ«å¤§ï¼ŒDeepSeek ä¼šè‡ªåŠ¨å¤„ç†æˆ–æŠ¥é”™ï¼Œæˆ‘ä»¬åšä¸ªå®‰å…¨æˆªæ–­
    safe_text = full_text[:100000]

    prompt = """
    ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æå¸ˆã€‚è¿™ç¯‡æ–‡æ¡£é‡‡ç”¨äº†â€œé¢˜ç›®ä¸ç­”æ¡ˆäº¤é”™â€çš„æ’ç‰ˆæ–¹å¼ï¼ˆä¾‹å¦‚ï¼š50é“é¢˜ -> 50ä¸ªç­”æ¡ˆ -> 50é“é¢˜...ï¼‰ã€‚

    ã€ä»»åŠ¡ã€‘
    è¯·é€šè¯»å…¨æ–‡ï¼Œå°†åˆ†æ•£åœ¨æ–‡æ¡£å„ä¸ªä½ç½®çš„â€œå‚è€ƒç­”æ¡ˆâ€å…¨éƒ¨æå–å‡ºæ¥ï¼Œåˆå¹¶æˆä¸€ä¸ªâ€œæ€»ç­”æ¡ˆè¡¨â€ã€‚

    ã€è¾“å‡ºæ ¼å¼ã€‘
    è¯·ç›´æ¥è¾“å‡ºç­”æ¡ˆåˆ—è¡¨ï¼Œæ ¼å¼ä¸ºï¼š
    1. A
    2. B
    ...

    ä¸è¦åŒ…å«é¢˜ç›®å†…å®¹ï¼Œåªè¦ç­”æ¡ˆã€‚å¦‚æœæ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè¿”å›"æ— ç­”æ¡ˆ"ã€‚
    """

    try:
        response = client.chat.completions.create(
            model=AI_MODEL_NAME,
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": safe_text}
            ],
            temperature=0.1,
            stream=False
        )
        ans = response.choices[0].message.content
        print(f"   âœ… å‚è€ƒç­”æ¡ˆåº“æ„å»ºå®Œæˆ (é•¿åº¦: {len(ans)} å­—ç¬¦)")
        return ans
    except Exception as e:
        print(f"   âš ï¸ ç­”æ¡ˆæå–å¤±è´¥: {e}")
        return ""


def process_single_chunk(args):
    chunk, index, total, answer_key = args

    # åŠ¨æ€è£å‰ª Answer Keyï¼Œåªä¿ç•™ç›¸å…³çš„éƒ¨åˆ†ç»™åˆ‡ç‰‡ï¼ˆèŠ‚çœTokenï¼‰
    # è¿™é‡Œç®€å•å¤„ç†ï¼šå¦‚æœ Answer Key å¾ˆå¤§ï¼Œåªä¼ å‰ 10000 å­—ç¬¦ã€‚
    # æ›´ä¼˜åšæ³•æ˜¯è®© DeepSeek è‡ªå·±åœ¨å…¨æ–‡é‡Œæ‰¾ï¼Œä½†åœ¨åˆ‡ç‰‡é˜¶æ®µæˆ‘ä»¬åªèƒ½ç»™å®ƒâ€œå­—å…¸â€
    # å¯¹äº DeepSeekï¼Œæˆ‘ä»¬å¯ä»¥ç¨å¾®ç»™å¤šç‚¹ä¸Šä¸‹æ–‡ã€‚

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªè¯•é¢˜æå–ä¸“å®¶ã€‚è¯·å°†æ–‡æœ¬åˆ‡ç‰‡è½¬æ¢ä¸º JSON æ•°ç»„ã€‚

    ### å…¨å±€å‚è€ƒç­”æ¡ˆåº“ (Global Answer Key)
    --------------------------------------------------
    {answer_key[:15000]} ... (ç­”æ¡ˆåº“ç‰‡æ®µ)
    --------------------------------------------------

    ### ä»»åŠ¡è¦æ±‚
    1. **æå–é¢˜ç›®**ï¼šå¿½ç•¥åˆ‡ç‰‡é¦–å°¾ä¸å®Œæ•´çš„æ®‹ç¼ºå¥ã€‚
    2. **é…å¯¹ç­”æ¡ˆ**ï¼š
       - æå–é¢˜ç›®åï¼ŒæŸ¥çœ‹å…¶ã€é¢˜å·ã€‘ã€‚
       - åœ¨ä¸Šæ–¹çš„ã€å…¨å±€å‚è€ƒç­”æ¡ˆåº“ã€‘ä¸­æŸ¥æ‰¾å¯¹åº”é¢˜å·çš„ç­”æ¡ˆã€‚
       - å¦‚æœé¢˜ç›®æ–‡å­—é™„è¿‘è‡ªå¸¦ç­”æ¡ˆï¼Œä¼˜å…ˆä½¿ç”¨è‡ªå¸¦ç­”æ¡ˆã€‚
       - **å¿…é¡»å¡«å…¥ answer å­—æ®µ**ã€‚
    3. **æ¨æ–­ç±»å‹**ï¼šè‡ªåŠ¨åˆ¤æ–­ category å’Œ typeã€‚

    ### JSON è¾“å‡ºæ ¼å¼
    [
      {{
        "category": "å•é€‰é¢˜",
        "type": "SINGLE_CHOICE", 
        "content": "é¢˜å¹²å†…å®¹...", 
        "options": [{{"label":"A", "text":"..."}}], 
        "answer": "A",
        "analysis": ""
      }}
    ]
    """

    for attempt in range(MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model=AI_MODEL_NAME,
                messages=[{"role": "system", "content": prompt}, {"role": "user", "content": chunk}],
                temperature=0.0,  # ç»å¯¹ç†æ™º
                max_tokens=4000
            )
            content = response.choices[0].message.content

            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            content = content.strip()

            try:
                return json.loads(content)
            except json.JSONDecodeError:
                fixed = repair_json(content)
                return json.loads(fixed)

        except Exception as e:
            wait_time = (2 ** attempt) + random.uniform(0, 1)
            time.sleep(wait_time)

    print(f"âŒ Chunk {index + 1} å½»åº•å¤±è´¥ã€‚")
    return []


def main():
    start_time = time.time()

    if not os.path.exists(INPUT_DIR): os.makedirs(INPUT_DIR)
    docx_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(".docx")]

    if not docx_files:
        print("âŒ input ç›®å½•ä¸ºç©ºã€‚")
        return

    all_questions = []
    seen_hashes = set()

    print(f"ğŸš€ DeepSeek-V3 å¼•æ“å¯åŠ¨ | å¹¶å‘: {MAX_WORKERS} | æ–‡æ¡£æ•°: {len(docx_files)}")

    for filename in docx_files:
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        raw_text = read_docx(os.path.join(INPUT_DIR, filename))
        if not raw_text: continue

        # 1. æå–ç­”æ¡ˆ (å…¨æ–‡æ‰«æ)
        global_answers = extract_global_answers(raw_text)

        # 2. åˆ‡ç‰‡
        chunks = get_chunks(raw_text, CHUNK_SIZE, OVERLAP)

        # 3. å¹¶å‘å¤„ç†
        tasks_args = [(chunk, i, len(chunks), global_answers) for i, chunk in enumerate(chunks)]

        chunk_added = 0
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            results = list(tqdm(executor.map(process_single_chunk, tasks_args), total=len(chunks), unit="åˆ‡ç‰‡"))

            for items in results:
                if items:
                    for item in items:
                        fp = generate_fingerprint(item)
                        if fp in seen_hashes: continue
                        seen_hashes.add(fp)

                        item['category'] = normalize_category(item.get('category', 'ç»¼åˆé¢˜'))
                        item['id'] = str(uuid.uuid4())
                        item['number'] = len(all_questions) + 1
                        item['chapter'] = filename.replace(".docx", "")
                        all_questions.append(item)
                        chunk_added += 1

        print(f"   âœ… æå–å®Œæˆ: {chunk_added} é“é¢˜")

    final_json = {
        "version": "DeepSeek-Interleaved",
        "total_count": len(all_questions),
        "data": all_questions
    }

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    out_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(final_json, f, ensure_ascii=False, indent=2)

    duration = time.time() - start_time
    msg = f"DeepSeek å¤„ç†å®Œæˆï¼\nè€—æ—¶: {duration:.1f}s\né¢˜ç›®: {len(all_questions)}"
    print(f"\nâœ¨ {msg}")
    send_notification("âœ… DeepSeek é¢˜åº“è½¬æ¢æˆåŠŸ", msg.replace('\n', '<br>'))


if __name__ == "__main__":
    main()